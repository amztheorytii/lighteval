[2025-07-09 15:22:45,261] [[32m    INFO[0m]: PyTorch version 2.7.0 available. (config.py:54)[0m
INFO 07-09 15:22:49 [__init__.py:244] Automatically detected platform cuda.
[2025-07-09 15:22:52,359] [[32m    INFO[0m]: --- LOADING MODEL --- (pipeline.py:186)[0m
[2025-07-09 15:23:03,640] [[32m    INFO[0m]: This model supports multiple tasks: {'embed', 'classify', 'score', 'generate', 'reward'}. Defaulting to 'generate'. (config.py:823)[0m
[2025-07-09 15:23:04,562] [[32m    INFO[0m]: Defaulting to use mp for distributed inference (config.py:1946)[0m
[2025-07-09 15:23:04,562] [[32m    INFO[0m]: Chunked prefill is enabled with max_num_batched_tokens=2048. (config.py:2195)[0m
WARNING 07-09 15:23:08 [env_override.py:17] NCCL_CUMEM_ENABLE is set to 0, skipping override. This may increase memory overhead with cudagraph+allreduce: https://github.com/NVIDIA/nccl/issues/1234
INFO 07-09 15:23:10 [__init__.py:244] Automatically detected platform cuda.
INFO 07-09 15:23:13 [core.py:455] Waiting for init message from front-end.
INFO 07-09 15:23:13 [core.py:70] Initializing a V1 LLM engine (v0.9.1) with config: model='Qwen/Qwen2.5-7B', speculative_config=None, tokenizer='Qwen/Qwen2.5-7B', skip_tokenizer_init=False, tokenizer_mode=auto, revision=main, override_neuron_config={}, tokenizer_revision=main, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=4, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=1234, served_model_name=Qwen/Qwen2.5-7B, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=True, pooler_config=None, compilation_config={"level":3,"debug_dump_path":"","cache_dir":"","backend":"","custom_ops":["none"],"splitting_ops":["vllm.unified_attention","vllm.unified_attention_with_output"],"use_inductor":true,"compile_sizes":[],"inductor_compile_config":{"enable_auto_functionalized_v2":false},"inductor_passes":{},"use_cudagraph":true,"cudagraph_num_of_warmups":1,"cudagraph_capture_sizes":[512,504,496,488,480,472,464,456,448,440,432,424,416,408,400,392,384,376,368,360,352,344,336,328,320,312,304,296,288,280,272,264,256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"cudagraph_copy_inputs":false,"full_cuda_graph":false,"max_capture_size":512,"local_cache_dir":null}
WARNING 07-09 15:23:13 [multiproc_worker_utils.py:307] Reducing Torch parallelism from 128 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.
INFO 07-09 15:23:13 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0, 1, 2, 3], buffer_handle=(4, 16777216, 10, 'psm_d59ec673'), local_subscribe_addr='ipc:///tmp/0562fcae-3c2b-4c0f-8b0a-da78c1ac0c72', remote_subscribe_addr=None, remote_addr_ipv6=False)
WARNING 07-09 15:23:15 [env_override.py:17] NCCL_CUMEM_ENABLE is set to 0, skipping override. This may increase memory overhead with cudagraph+allreduce: https://github.com/NVIDIA/nccl/issues/1234
WARNING 07-09 15:23:15 [env_override.py:17] NCCL_CUMEM_ENABLE is set to 0, skipping override. This may increase memory overhead with cudagraph+allreduce: https://github.com/NVIDIA/nccl/issues/1234
WARNING 07-09 15:23:15 [env_override.py:17] NCCL_CUMEM_ENABLE is set to 0, skipping override. This may increase memory overhead with cudagraph+allreduce: https://github.com/NVIDIA/nccl/issues/1234
WARNING 07-09 15:23:15 [env_override.py:17] NCCL_CUMEM_ENABLE is set to 0, skipping override. This may increase memory overhead with cudagraph+allreduce: https://github.com/NVIDIA/nccl/issues/1234
INFO 07-09 15:23:17 [__init__.py:244] Automatically detected platform cuda.
INFO 07-09 15:23:17 [__init__.py:244] Automatically detected platform cuda.
INFO 07-09 15:23:17 [__init__.py:244] Automatically detected platform cuda.
INFO 07-09 15:23:17 [__init__.py:244] Automatically detected platform cuda.
WARNING 07-09 15:23:21 [utils.py:2737] Methods determine_num_available_blocks,device_config,get_cache_block_size_bytes,initialize_cache not implemented in <vllm.v1.worker.gpu_worker.Worker object at 0x7f05af8e1d10>
[1;36m(VllmWorker rank=0 pid=1372027)[0;0m INFO 07-09 15:23:21 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_3eadc0f9'), local_subscribe_addr='ipc:///tmp/08ef2273-a4c4-48e1-a014-e71fa2fefb5e', remote_subscribe_addr=None, remote_addr_ipv6=False)
WARNING 07-09 15:23:21 [utils.py:2737] Methods determine_num_available_blocks,device_config,get_cache_block_size_bytes,initialize_cache not implemented in <vllm.v1.worker.gpu_worker.Worker object at 0x7f28cf3ea710>
[1;36m(VllmWorker rank=3 pid=1372030)[0;0m INFO 07-09 15:23:21 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_9d95a0ef'), local_subscribe_addr='ipc:///tmp/5a8f9111-1f39-4496-9002-fd60ece8d2b5', remote_subscribe_addr=None, remote_addr_ipv6=False)
WARNING 07-09 15:23:21 [utils.py:2737] Methods determine_num_available_blocks,device_config,get_cache_block_size_bytes,initialize_cache not implemented in <vllm.v1.worker.gpu_worker.Worker object at 0x7f9a58c8ba90>
[1;36m(VllmWorker rank=1 pid=1372028)[0;0m INFO 07-09 15:23:21 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_ff42b009'), local_subscribe_addr='ipc:///tmp/12309f37-6dac-46e4-951d-ec226e9b4da1', remote_subscribe_addr=None, remote_addr_ipv6=False)
WARNING 07-09 15:23:21 [utils.py:2737] Methods determine_num_available_blocks,device_config,get_cache_block_size_bytes,initialize_cache not implemented in <vllm.v1.worker.gpu_worker.Worker object at 0x7f4bee5e0b10>
[1;36m(VllmWorker rank=2 pid=1372029)[0;0m INFO 07-09 15:23:21 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_d5506883'), local_subscribe_addr='ipc:///tmp/054c971d-0ac5-4638-833a-e5d0ae429617', remote_subscribe_addr=None, remote_addr_ipv6=False)
[1;36m(VllmWorker rank=2 pid=1372029)[0;0m INFO 07-09 15:23:23 [utils.py:1126] Found nccl from library libnccl.so.2
[1;36m(VllmWorker rank=3 pid=1372030)[0;0m INFO 07-09 15:23:23 [utils.py:1126] Found nccl from library libnccl.so.2
[1;36m(VllmWorker rank=1 pid=1372028)[0;0m INFO 07-09 15:23:23 [utils.py:1126] Found nccl from library libnccl.so.2
[1;36m(VllmWorker rank=2 pid=1372029)[0;0m INFO 07-09 15:23:23 [pynccl.py:70] vLLM is using nccl==2.26.2
[1;36m(VllmWorker rank=1 pid=1372028)[0;0m INFO 07-09 15:23:23 [pynccl.py:70] vLLM is using nccl==2.26.2
[1;36m(VllmWorker rank=3 pid=1372030)[0;0m INFO 07-09 15:23:23 [pynccl.py:70] vLLM is using nccl==2.26.2
[1;36m(VllmWorker rank=0 pid=1372027)[0;0m INFO 07-09 15:23:23 [utils.py:1126] Found nccl from library libnccl.so.2
[1;36m(VllmWorker rank=0 pid=1372027)[0;0m INFO 07-09 15:23:23 [pynccl.py:70] vLLM is using nccl==2.26.2
[1;36m(VllmWorker rank=3 pid=1372030)[0;0m INFO 07-09 15:23:24 [custom_all_reduce_utils.py:246] reading GPU P2P access cache from /lustre1/tier2/users/ahmed.alzubaidi/.cache/vllm/gpu_p2p_access_cache_for_0,1,2,3,4,5,6,7.json
[1;36m(VllmWorker rank=0 pid=1372027)[0;0m INFO 07-09 15:23:24 [custom_all_reduce_utils.py:246] reading GPU P2P access cache from /lustre1/tier2/users/ahmed.alzubaidi/.cache/vllm/gpu_p2p_access_cache_for_0,1,2,3,4,5,6,7.json
[1;36m(VllmWorker rank=2 pid=1372029)[0;0m INFO 07-09 15:23:24 [custom_all_reduce_utils.py:246] reading GPU P2P access cache from /lustre1/tier2/users/ahmed.alzubaidi/.cache/vllm/gpu_p2p_access_cache_for_0,1,2,3,4,5,6,7.json
[1;36m(VllmWorker rank=1 pid=1372028)[0;0m INFO 07-09 15:23:24 [custom_all_reduce_utils.py:246] reading GPU P2P access cache from /lustre1/tier2/users/ahmed.alzubaidi/.cache/vllm/gpu_p2p_access_cache_for_0,1,2,3,4,5,6,7.json
[1;36m(VllmWorker rank=0 pid=1372027)[0;0m INFO 07-09 15:23:24 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[1, 2, 3], buffer_handle=(3, 4194304, 6, 'psm_20fc3778'), local_subscribe_addr='ipc:///tmp/169ff232-7770-4a78-b748-d1e90dd47d81', remote_subscribe_addr=None, remote_addr_ipv6=False)
[1;36m(VllmWorker rank=0 pid=1372027)[0;0m INFO 07-09 15:23:24 [parallel_state.py:1065] rank 0 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
[1;36m(VllmWorker rank=2 pid=1372029)[0;0m INFO 07-09 15:23:24 [parallel_state.py:1065] rank 2 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 2, EP rank 2
[1;36m(VllmWorker rank=1 pid=1372028)[0;0m INFO 07-09 15:23:24 [parallel_state.py:1065] rank 1 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 1, EP rank 1
[1;36m(VllmWorker rank=3 pid=1372030)[0;0m INFO 07-09 15:23:24 [parallel_state.py:1065] rank 3 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 3, EP rank 3
[1;36m(VllmWorker rank=1 pid=1372028)[0;0m WARNING 07-09 15:23:24 [topk_topp_sampler.py:59] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.
[1;36m(VllmWorker rank=3 pid=1372030)[0;0m WARNING 07-09 15:23:24 [topk_topp_sampler.py:59] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.
[1;36m(VllmWorker rank=2 pid=1372029)[0;0m WARNING 07-09 15:23:24 [topk_topp_sampler.py:59] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.
[1;36m(VllmWorker rank=0 pid=1372027)[0;0m WARNING 07-09 15:23:24 [topk_topp_sampler.py:59] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.
[1;36m(VllmWorker rank=3 pid=1372030)[0;0m INFO 07-09 15:23:24 [gpu_model_runner.py:1595] Starting to load model Qwen/Qwen2.5-7B...
[1;36m(VllmWorker rank=1 pid=1372028)[0;0m INFO 07-09 15:23:24 [gpu_model_runner.py:1595] Starting to load model Qwen/Qwen2.5-7B...
[1;36m(VllmWorker rank=2 pid=1372029)[0;0m INFO 07-09 15:23:24 [gpu_model_runner.py:1595] Starting to load model Qwen/Qwen2.5-7B...
[1;36m(VllmWorker rank=0 pid=1372027)[0;0m INFO 07-09 15:23:24 [gpu_model_runner.py:1595] Starting to load model Qwen/Qwen2.5-7B...
[1;36m(VllmWorker rank=0 pid=1372027)[0;0m INFO 07-09 15:23:24 [gpu_model_runner.py:1600] Loading model from scratch...
[1;36m(VllmWorker rank=2 pid=1372029)[0;0m INFO 07-09 15:23:24 [gpu_model_runner.py:1600] Loading model from scratch...
[1;36m(VllmWorker rank=1 pid=1372028)[0;0m INFO 07-09 15:23:24 [gpu_model_runner.py:1600] Loading model from scratch...
[1;36m(VllmWorker rank=3 pid=1372030)[0;0m INFO 07-09 15:23:24 [gpu_model_runner.py:1600] Loading model from scratch...
[1;36m(VllmWorker rank=0 pid=1372027)[0;0m INFO 07-09 15:23:25 [cuda.py:252] Using Flash Attention backend on V1 engine.
[1;36m(VllmWorker rank=2 pid=1372029)[0;0m INFO 07-09 15:23:25 [cuda.py:252] Using Flash Attention backend on V1 engine.
[1;36m(VllmWorker rank=1 pid=1372028)[0;0m INFO 07-09 15:23:25 [cuda.py:252] Using Flash Attention backend on V1 engine.
[1;36m(VllmWorker rank=3 pid=1372030)[0;0m INFO 07-09 15:23:25 [cuda.py:252] Using Flash Attention backend on V1 engine.
[1;36m(VllmWorker rank=3 pid=1372030)[0;0m INFO 07-09 15:23:25 [weight_utils.py:292] Using model weights format ['*.safetensors']
[1;36m(VllmWorker rank=2 pid=1372029)[0;0m INFO 07-09 15:23:25 [weight_utils.py:292] Using model weights format ['*.safetensors']
[1;36m(VllmWorker rank=1 pid=1372028)[0;0m INFO 07-09 15:23:25 [weight_utils.py:292] Using model weights format ['*.safetensors']
[1;36m(VllmWorker rank=0 pid=1372027)[0;0m INFO 07-09 15:23:26 [weight_utils.py:292] Using model weights format ['*.safetensors']
[1;36m(VllmWorker rank=0 pid=1372027)[0;0m Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
[1;36m(VllmWorker rank=0 pid=1372027)[0;0m Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:00<00:00,  3.03it/s]
[1;36m(VllmWorker rank=3 pid=1372030)[0;0m INFO 07-09 15:23:28 [default_loader.py:272] Loading weights took 1.50 seconds
[1;36m(VllmWorker rank=0 pid=1372027)[0;0m Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:00<00:00,  2.65it/s]
[1;36m(VllmWorker rank=3 pid=1372030)[0;0m INFO 07-09 15:23:28 [gpu_model_runner.py:1624] Model loading took 3.5783 GiB and 3.024000 seconds
[1;36m(VllmWorker rank=0 pid=1372027)[0;0m Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:01<00:00,  2.75it/s]
[1;36m(VllmWorker rank=0 pid=1372027)[0;0m Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:01<00:00,  2.62it/s]
[1;36m(VllmWorker rank=0 pid=1372027)[0;0m Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:01<00:00,  2.67it/s]
[1;36m(VllmWorker rank=0 pid=1372027)[0;0m 
[1;36m(VllmWorker rank=0 pid=1372027)[0;0m INFO 07-09 15:23:29 [default_loader.py:272] Loading weights took 1.52 seconds
[1;36m(VllmWorker rank=0 pid=1372027)[0;0m INFO 07-09 15:23:29 [gpu_model_runner.py:1624] Model loading took 3.5783 GiB and 4.126520 seconds
[1;36m(VllmWorker rank=1 pid=1372028)[0;0m INFO 07-09 15:23:29 [default_loader.py:272] Loading weights took 1.56 seconds
[1;36m(VllmWorker rank=2 pid=1372029)[0;0m INFO 07-09 15:23:30 [default_loader.py:272] Loading weights took 1.45 seconds
[1;36m(VllmWorker rank=1 pid=1372028)[0;0m INFO 07-09 15:23:30 [gpu_model_runner.py:1624] Model loading took 3.5783 GiB and 4.901641 seconds
[1;36m(VllmWorker rank=2 pid=1372029)[0;0m INFO 07-09 15:23:30 [gpu_model_runner.py:1624] Model loading took 3.5783 GiB and 5.386707 seconds
[1;36m(VllmWorker rank=0 pid=1372027)[0;0m INFO 07-09 15:23:36 [backends.py:462] Using cache directory: /lustre1/tier2/users/ahmed.alzubaidi/.cache/vllm/torch_compile_cache/8481a0be4b/rank_0_0 for vLLM's torch.compile
[1;36m(VllmWorker rank=0 pid=1372027)[0;0m INFO 07-09 15:23:36 [backends.py:472] Dynamo bytecode transform time: 5.48 s
[1;36m(VllmWorker rank=2 pid=1372029)[0;0m INFO 07-09 15:23:36 [backends.py:462] Using cache directory: /lustre1/tier2/users/ahmed.alzubaidi/.cache/vllm/torch_compile_cache/8481a0be4b/rank_2_0 for vLLM's torch.compile
[1;36m(VllmWorker rank=2 pid=1372029)[0;0m INFO 07-09 15:23:36 [backends.py:472] Dynamo bytecode transform time: 5.46 s
[1;36m(VllmWorker rank=3 pid=1372030)[0;0m INFO 07-09 15:23:36 [backends.py:462] Using cache directory: /lustre1/tier2/users/ahmed.alzubaidi/.cache/vllm/torch_compile_cache/8481a0be4b/rank_3_0 for vLLM's torch.compile
[1;36m(VllmWorker rank=3 pid=1372030)[0;0m INFO 07-09 15:23:36 [backends.py:472] Dynamo bytecode transform time: 5.51 s
[1;36m(VllmWorker rank=1 pid=1372028)[0;0m INFO 07-09 15:23:36 [backends.py:462] Using cache directory: /lustre1/tier2/users/ahmed.alzubaidi/.cache/vllm/torch_compile_cache/8481a0be4b/rank_1_0 for vLLM's torch.compile
[1;36m(VllmWorker rank=1 pid=1372028)[0;0m INFO 07-09 15:23:36 [backends.py:472] Dynamo bytecode transform time: 5.54 s
[1;36m(VllmWorker rank=0 pid=1372027)[0;0m INFO 07-09 15:23:41 [backends.py:135] Directly load the compiled graph(s) for shape None from the cache, took 4.587 s
[1;36m(VllmWorker rank=2 pid=1372029)[0;0m INFO 07-09 15:23:41 [backends.py:135] Directly load the compiled graph(s) for shape None from the cache, took 4.648 s
[1;36m(VllmWorker rank=3 pid=1372030)[0;0m INFO 07-09 15:23:41 [backends.py:135] Directly load the compiled graph(s) for shape None from the cache, took 4.671 s
[1;36m(VllmWorker rank=1 pid=1372028)[0;0m INFO 07-09 15:23:41 [backends.py:135] Directly load the compiled graph(s) for shape None from the cache, took 4.688 s
[1;36m(VllmWorker rank=2 pid=1372029)[0;0m INFO 07-09 15:23:43 [monitor.py:34] torch.compile takes 5.46 s in total
[1;36m(VllmWorker rank=3 pid=1372030)[0;0m INFO 07-09 15:23:43 [monitor.py:34] torch.compile takes 5.51 s in total
[1;36m(VllmWorker rank=0 pid=1372027)[0;0m INFO 07-09 15:23:43 [monitor.py:34] torch.compile takes 5.48 s in total
[1;36m(VllmWorker rank=1 pid=1372028)[0;0m INFO 07-09 15:23:43 [monitor.py:34] torch.compile takes 5.54 s in total
[1;36m(VllmWorker rank=0 pid=1372027)[0;0m INFO 07-09 15:23:44 [gpu_worker.py:227] Available KV cache memory: 65.39 GiB
[1;36m(VllmWorker rank=3 pid=1372030)[0;0m INFO 07-09 15:23:44 [gpu_worker.py:227] Available KV cache memory: 65.39 GiB
[1;36m(VllmWorker rank=2 pid=1372029)[0;0m INFO 07-09 15:23:44 [gpu_worker.py:227] Available KV cache memory: 65.11 GiB
[1;36m(VllmWorker rank=1 pid=1372028)[0;0m INFO 07-09 15:23:44 [gpu_worker.py:227] Available KV cache memory: 65.11 GiB
INFO 07-09 15:23:44 [kv_cache_utils.py:715] GPU KV cache size: 4,897,792 tokens
INFO 07-09 15:23:44 [kv_cache_utils.py:719] Maximum concurrency for 131,072 tokens per request: 37.37x
INFO 07-09 15:23:44 [kv_cache_utils.py:715] GPU KV cache size: 4,876,736 tokens
INFO 07-09 15:23:44 [kv_cache_utils.py:719] Maximum concurrency for 131,072 tokens per request: 37.21x
INFO 07-09 15:23:44 [kv_cache_utils.py:715] GPU KV cache size: 4,876,736 tokens
INFO 07-09 15:23:44 [kv_cache_utils.py:719] Maximum concurrency for 131,072 tokens per request: 37.21x
INFO 07-09 15:23:44 [kv_cache_utils.py:715] GPU KV cache size: 4,897,792 tokens
INFO 07-09 15:23:44 [kv_cache_utils.py:719] Maximum concurrency for 131,072 tokens per request: 37.37x
[1;36m(VllmWorker rank=0 pid=1372027)[0;0m INFO 07-09 15:24:05 [custom_all_reduce.py:196] Registering 3819 cuda graph addresses
[1;36m(VllmWorker rank=3 pid=1372030)[0;0m INFO 07-09 15:24:05 [custom_all_reduce.py:196] Registering 3819 cuda graph addresses
[1;36m(VllmWorker rank=2 pid=1372029)[0;0m INFO 07-09 15:24:08 [custom_all_reduce.py:196] Registering 3819 cuda graph addresses
[1;36m(VllmWorker rank=1 pid=1372028)[0;0m INFO 07-09 15:24:08 [custom_all_reduce.py:196] Registering 3819 cuda graph addresses
[1;36m(VllmWorker rank=3 pid=1372030)[0;0m INFO 07-09 15:24:08 [gpu_model_runner.py:2048] Graph capturing finished in 23 secs, took 1.60 GiB
[1;36m(VllmWorker rank=2 pid=1372029)[0;0m INFO 07-09 15:24:08 [gpu_model_runner.py:2048] Graph capturing finished in 23 secs, took 1.60 GiB
[1;36m(VllmWorker rank=0 pid=1372027)[0;0m INFO 07-09 15:24:08 [gpu_model_runner.py:2048] Graph capturing finished in 23 secs, took 1.60 GiB
[1;36m(VllmWorker rank=1 pid=1372028)[0;0m INFO 07-09 15:24:08 [gpu_model_runner.py:2048] Graph capturing finished in 23 secs, took 1.60 GiB
INFO 07-09 15:24:08 [core.py:171] init engine (profile, create kv cache, warmup model) took 37.56 seconds
[2025-07-09 15:24:09,566] [[32m    INFO[0m]: --- INIT SEEDS --- (pipeline.py:253)[0m
[2025-07-09 15:24:09,566] [[32m    INFO[0m]: --- LOADING TASKS --- (pipeline.py:210)[0m
[2025-07-09 15:24:09,620] [[33m WARNING[0m]: If you want to use extended_tasks, make sure you installed their dependencies using `pip install -e .[extended_tasks]`. (registry.py:138)[0m
[2025-07-09 15:24:09,620] [[32m    INFO[0m]: Found 245 custom tasks in /lustre1/tier2/projects/falcon-arabic/.cache/huggingface/modules/datasets_modules/datasets/arabic_evals/b602e1b4ea346d798578dbc281c9a720e755e2ed31ba9dda81f2d4e7f87457f8/arabic_evals.py (registry.py:142)[0m
[2025-07-09 15:26:04,294] [[32m    INFO[0m]: --- RUNNING MODEL --- (pipeline.py:332)[0m
[2025-07-09 15:26:04,294] [[32m    INFO[0m]: Running SamplingMethod.LOGPROBS requests (pipeline.py:315)[0m
0it [00:00, ?it/s]
Adding requests:   0%|          | 0/2148 [00:00<?, ?it/s][A
Adding requests:  62%|██████▏   | 1338/2148 [00:00<00:00, 13377.52it/s][AAdding requests: 100%|██████████| 2148/2148 [00:00<00:00, 13461.46it/s]

Processed prompts:   0%|          | 0/2148 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s][A
Processed prompts:   1%|          | 21/2148 [00:00<00:18, 117.21it/s, est. speed input: 24934.93 toks/s, output: 117.21 toks/s][A
Processed prompts:   2%|▏         | 41/2148 [00:00<00:17, 120.12it/s, est. speed input: 25138.86 toks/s, output: 119.66 toks/s][A
Processed prompts:   3%|▎         | 65/2148 [00:00<00:15, 131.77it/s, est. speed input: 24958.88 toks/s, output: 128.23 toks/s][A
Processed prompts:   4%|▍         | 90/2148 [00:00<00:14, 138.67it/s, est. speed input: 24825.45 toks/s, output: 133.45 toks/s][A
Processed prompts:   5%|▌         | 115/2148 [00:00<00:14, 143.52it/s, est. speed input: 24914.23 toks/s, output: 137.14 toks/s][A
Processed prompts:   6%|▋         | 138/2148 [00:01<00:14, 142.50it/s, est. speed input: 24883.75 toks/s, output: 137.69 toks/s][A
Processed prompts:   8%|▊         | 164/2148 [00:01<00:13, 145.85it/s, est. speed input: 24810.73 toks/s, output: 139.84 toks/s][A
Processed prompts:   9%|▉         | 192/2148 [00:01<00:12, 152.25it/s, est. speed input: 24651.68 toks/s, output: 143.11 toks/s][A
Processed prompts:  10%|█         | 220/2148 [00:01<00:12, 157.20it/s, est. speed input: 24688.19 toks/s, output: 145.88 toks/s][A
Processed prompts:  11%|█▏        | 247/2148 [00:01<00:12, 158.38it/s, est. speed input: 24693.53 toks/s, output: 147.39 toks/s][A
Processed prompts:  13%|█▎        | 273/2148 [00:01<00:11, 157.14it/s, est. speed input: 24619.20 toks/s, output: 148.03 toks/s][A
Processed prompts:  14%|█▍        | 302/2148 [00:02<00:11, 161.31it/s, est. speed input: 24555.02 toks/s, output: 149.94 toks/s][A
Processed prompts:  15%|█▌        | 329/2148 [00:02<00:11, 161.21it/s, est. speed input: 24545.25 toks/s, output: 150.79 toks/s][A
Processed prompts:  16%|█▌        | 346/2148 [00:02<00:13, 136.17it/s, est. speed input: 23562.43 toks/s, output: 145.49 toks/s][A
Processed prompts:  18%|█▊        | 388/2148 [00:02<00:09, 176.07it/s, est. speed input: 24509.80 toks/s, output: 153.73 toks/s][A
Processed prompts:  19%|█▉        | 417/2148 [00:03<00:18, 95.53it/s, est. speed input: 20898.04 toks/s, output: 132.17 toks/s] [A
Processed prompts:  24%|██▍       | 522/2148 [00:03<00:08, 202.96it/s, est. speed input: 24353.27 toks/s, output: 158.44 toks/s][A
Processed prompts:  26%|██▌       | 555/2148 [00:03<00:07, 200.39it/s, est. speed input: 24379.91 toks/s, output: 160.08 toks/s][A
Processed prompts:  27%|██▋       | 584/2148 [00:03<00:08, 192.69it/s, est. speed input: 24293.80 toks/s, output: 160.49 toks/s][A
Processed prompts:  29%|██▊       | 616/2148 [00:03<00:08, 190.92it/s, est. speed input: 24264.47 toks/s, output: 161.63 toks/s][A
Processed prompts:  30%|███       | 649/2148 [00:03<00:07, 190.34it/s, est. speed input: 24226.52 toks/s, output: 162.82 toks/s][A
Processed prompts:  32%|███▏      | 681/2148 [00:04<00:07, 189.32it/s, est. speed input: 24226.35 toks/s, output: 163.80 toks/s][A
Processed prompts:  33%|███▎      | 714/2148 [00:04<00:07, 189.86it/s, est. speed input: 24199.50 toks/s, output: 164.90 toks/s][A
Processed prompts:  35%|███▍      | 745/2148 [00:04<00:07, 186.46it/s, est. speed input: 24189.92 toks/s, output: 165.41 toks/s][A
Processed prompts:  36%|███▌      | 778/2148 [00:04<00:07, 187.09it/s, est. speed input: 24141.49 toks/s, output: 166.27 toks/s][A
Processed prompts:  38%|███▊      | 811/2148 [00:04<00:07, 188.13it/s, est. speed input: 24140.91 toks/s, output: 167.14 toks/s][A
Processed prompts:  39%|███▉      | 845/2148 [00:05<00:06, 189.87it/s, est. speed input: 24104.31 toks/s, output: 168.08 toks/s][A
Processed prompts:  41%|████      | 881/2148 [00:05<00:06, 194.50it/s, est. speed input: 24090.62 toks/s, output: 169.34 toks/s][A
Processed prompts:  43%|████▎     | 915/2148 [00:05<00:06, 194.02it/s, est. speed input: 24052.30 toks/s, output: 170.11 toks/s][A
Processed prompts:  44%|████▍     | 948/2148 [00:05<00:06, 192.23it/s, est. speed input: 24019.66 toks/s, output: 170.67 toks/s][A
Processed prompts:  46%|████▌     | 983/2148 [00:05<00:05, 194.82it/s, est. speed input: 24012.82 toks/s, output: 171.59 toks/s][A
Processed prompts:  47%|████▋     | 1018/2148 [00:05<00:05, 195.85it/s, est. speed input: 23991.10 toks/s, output: 172.39 toks/s][A
Processed prompts:  49%|████▉     | 1055/2148 [00:06<00:05, 199.81it/s, est. speed input: 23964.81 toks/s, output: 173.46 toks/s][A
Processed prompts:  51%|█████     | 1090/2148 [00:06<00:05, 199.08it/s, est. speed input: 23946.09 toks/s, output: 174.13 toks/s][A
Processed prompts:  52%|█████▏    | 1127/2148 [00:06<00:05, 201.64it/s, est. speed input: 23918.26 toks/s, output: 175.06 toks/s][A
Processed prompts:  54%|█████▍    | 1163/2148 [00:07<00:09, 107.09it/s, est. speed input: 22146.15 toks/s, output: 162.92 toks/s][A
Processed prompts:  60%|██████    | 1294/2148 [00:07<00:03, 243.37it/s, est. speed input: 23813.62 toks/s, output: 178.72 toks/s][A
Processed prompts:  63%|██████▎   | 1344/2148 [00:07<00:03, 250.62it/s, est. speed input: 23950.46 toks/s, output: 181.08 toks/s][A
Processed prompts:  65%|██████▍   | 1387/2148 [00:07<00:03, 247.95it/s, est. speed input: 23989.43 toks/s, output: 182.47 toks/s][A
Processed prompts:  66%|██████▋   | 1425/2148 [00:07<00:03, 238.33it/s, est. speed input: 23941.19 toks/s, output: 183.12 toks/s][A
Processed prompts:  68%|██████▊   | 1458/2148 [00:07<00:03, 223.33it/s, est. speed input: 23827.32 toks/s, output: 183.11 toks/s][A
Processed prompts:  69%|██████▉   | 1487/2148 [00:08<00:03, 206.33it/s, est. speed input: 23687.81 toks/s, output: 182.65 toks/s][A
Processed prompts:  71%|███████   | 1526/2148 [00:08<00:02, 208.72it/s, est. speed input: 23663.90 toks/s, output: 183.35 toks/s][A
Processed prompts:  73%|███████▎  | 1566/2148 [00:08<00:02, 212.64it/s, est. speed input: 23646.51 toks/s, output: 184.17 toks/s][A
Processed prompts:  75%|███████▍  | 1605/2148 [00:08<00:02, 213.36it/s, est. speed input: 23619.17 toks/s, output: 184.82 toks/s][A
Processed prompts:  77%|███████▋  | 1645/2148 [00:08<00:02, 215.27it/s, est. speed input: 23599.76 toks/s, output: 185.53 toks/s][A
Processed prompts:  79%|███████▊  | 1687/2148 [00:09<00:02, 219.55it/s, est. speed input: 23576.78 toks/s, output: 186.43 toks/s][A
Processed prompts:  80%|████████  | 1729/2148 [00:09<00:01, 222.39it/s, est. speed input: 23550.09 toks/s, output: 187.27 toks/s][A
Processed prompts:  82%|████████▏ | 1770/2148 [00:09<00:01, 223.15it/s, est. speed input: 23529.85 toks/s, output: 188.00 toks/s][A
Processed prompts:  84%|████████▍ | 1812/2148 [00:09<00:01, 225.09it/s, est. speed input: 23507.98 toks/s, output: 188.79 toks/s][A
Processed prompts:  86%|████████▋ | 1855/2148 [00:09<00:01, 227.12it/s, est. speed input: 23484.63 toks/s, output: 189.61 toks/s][A
Processed prompts:  88%|████████▊ | 1898/2148 [00:09<00:01, 229.26it/s, est. speed input: 23458.65 toks/s, output: 190.43 toks/s][A
Processed prompts:  90%|█████████ | 1940/2148 [00:10<00:00, 229.21it/s, est. speed input: 23434.53 toks/s, output: 191.13 toks/s][A
Processed prompts:  92%|█████████▏| 1984/2148 [00:10<00:00, 232.51it/s, est. speed input: 23419.02 toks/s, output: 192.00 toks/s][A
Processed prompts:  94%|█████████▍| 2028/2148 [00:10<00:00, 234.27it/s, est. speed input: 23397.41 toks/s, output: 192.81 toks/s][A
Processed prompts:  96%|█████████▋| 2072/2148 [00:10<00:00, 235.35it/s, est. speed input: 23375.72 toks/s, output: 193.59 toks/s][A
Processed prompts:  99%|█████████▊| 2117/2148 [00:10<00:00, 237.63it/s, est. speed input: 23352.20 toks/s, output: 194.43 toks/s][AProcessed prompts: 100%|██████████| 2148/2148 [00:10<00:00, 196.66it/s, est. speed input: 23529.94 toks/s, output: 196.66 toks/s]
1it [00:11, 11.66s/it]1it [00:11, 11.66s/it]
[2025-07-09 15:26:20,770] [[32m    INFO[0m]: --- COMPUTING METRICS --- (pipeline.py:358)[0m
[2025-07-09 15:26:20,790] [[32m    INFO[0m]: --- DISPLAYING RESULTS --- (pipeline.py:419)[0m
[2025-07-09 15:26:20,805] [[32m    INFO[0m]: --- SAVING AND PUSHING RESULTS --- (pipeline.py:409)[0m
[2025-07-09 15:26:20,805] [[32m    INFO[0m]: Saving experiment tracker (evaluation_tracker.py:216)[0m
[2025-07-09 15:26:21,063] [[32m    INFO[0m]: Saving results to /lustre1/tier2/projects/falcon-arabic/ahmed/falcon3-arabic/eval/modules/evaluators/lighteval_v2/results/results/Qwen/Qwen2.5-7B/results_2025-07-09T15-26-20.805840.json (evaluation_tracker.py:292)[0m
|          Task          |Version| Metric |Value |   |Stderr|
|------------------------|-------|--------|-----:|---|-----:|
|all                     |       |acc_norm|0.3855|±  | 0.021|
|community:arabic_exams:0|       |acc_norm|0.3855|±  | 0.021|

